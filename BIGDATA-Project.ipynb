{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6dca563",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#BigData---Final-Project\" data-toc-modified-id=\"BigData---Final-Project-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>BigData - Final Project</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-The-Data\" data-toc-modified-id=\"Loading-The-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Loading The Data</a></span></li><li><span><a href=\"#Exploring-The-Data\" data-toc-modified-id=\"Exploring-The-Data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Exploring The Data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d16f4d",
   "metadata": {},
   "source": [
    "# BigData - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce715a02",
   "metadata": {},
   "source": [
    "__AUTHORS__:\n",
    "  - Théo Perinet (22172 - theo.perinet)\n",
    "  - Mathieu Rivier (23553 - mathieu.rivier)\n",
    "  - Marc Monteil (23742 - marc.monteil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38335eff",
   "metadata": {},
   "source": [
    "###### To Use when you are on google collab\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz  \n",
    "!tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea659e8b",
   "metadata": {},
   "source": [
    "###### TO USE WHEN YOU ARE ON GOOGLE COLLAB\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80976068",
   "metadata": {},
   "source": [
    "## Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92cbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8e3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_application_name = \"WannaFlop_Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8bad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/SPARK/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/20 09:35:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0926b5",
   "metadata": {},
   "source": [
    "## Exploring The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33cf6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType, StructType,StructField\n",
    "from pyspark.sql.functions import desc\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56d2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class read_info(object):\n",
    "    def __init__(self, file_path, header=False, delimiter=';', schema=None):\n",
    "        self.file_path = file_path\n",
    "        self.header = header\n",
    "        self.delimiter = delimiter\n",
    "        self.schema = schema\n",
    "\n",
    "        self.df = self._load_df()\n",
    "\n",
    "        #self.df_abstract = self._get_df_abstract()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self._nb_rows()} \\n{self.df.printSchema()} \\n{self.get_df_abstract()}\\n {self.show_missing()}\\n{self._get_stats()}\"\n",
    "\n",
    "    def show_missing(self):\n",
    "        print(\"Missing Data per column:\")\n",
    "        self._count_missing().show()\n",
    "\n",
    "    def _get_num_cols(self):\n",
    "        num_cols = [\n",
    "            f.name for f in self.df.schema.fields\n",
    "            if isinstance(f.dataType, DoubleType) or\n",
    "            isinstance(f.dataType, IntegerType)\n",
    "        ]\n",
    "        \n",
    "        return num_cols\n",
    "    def _get_rounded_df(self):\n",
    "        rounded_df = self.df\n",
    "        dbl_cols = self._get_num_cols()\n",
    "        for col in dbl_cols:\n",
    "            rounded_df = rounded_df.withColumn(col, func.round('high'))\n",
    "\n",
    "        return rounded_df\n",
    "\n",
    "    def get_df_abstract(self):\n",
    "        rounded_df = self._get_rounded_df()\n",
    "\n",
    "        # First 40 rows\n",
    "        print(\"First 40 rows:\")\n",
    "        rounded_df.show(40)\n",
    "\n",
    "        # Last 40 rows\n",
    "        print(\"Last 40 rows:\")\n",
    "        rounded_df = rounded_df.withColumn(\n",
    "            \"index\", monotonically_increasing_id()\n",
    "        )\n",
    "        rounded_df.orderBy(desc(\"index\")).drop(\"index\").show(40)\n",
    "\n",
    "    def _get_periodicity(self):\n",
    "        self.df['data'][0]\n",
    "\n",
    "    def _nb_rows(self):\n",
    "        # Number of total rows\n",
    "        print(\"Number of rows: \" + str(self.df.count()) + \"\\n\")\n",
    "\n",
    "    def _handle_csv(self):\n",
    "        '''\n",
    "        @description: Read the csv file and return a Spark DataFrame\n",
    "\n",
    "        @arg csv_file_path: Path to the csv file\n",
    "        @arg header: boolean whether to load a header or not\n",
    "        @arg delimiter: which delimiter to use by default\n",
    "        '''\n",
    "        return spark.read.option(\"inferSchema\", \"true\").option(\"nullValue\", \"null\").csv(\n",
    "            self.file_path,\n",
    "            sep=self.delimiter,\n",
    "            schema=self.schema,\n",
    "            header=self.header,\n",
    "        )\n",
    "    \n",
    "    def _handle_json(self):\n",
    "        return spark.read.json(self.file_path)\n",
    "\n",
    "    def _load_df(self):\n",
    "        ####### ADD TRY CATCH #####\n",
    "        extension = self.file_path.split(\".\")[-1]\n",
    "\n",
    "        df = None\n",
    "        if extension == 'json':\n",
    "            df = self._handle_json()\n",
    "        elif extension == 'csv':\n",
    "            df = self._handle_csv()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _count_missing(self):\n",
    "        cols = self.df.columns\n",
    "        cols.remove('Date')\n",
    "        return self.df.select(\n",
    "            [\n",
    "                count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    "                for c in cols\n",
    "            ]\n",
    "        )\n",
    "        #.show()\n",
    "        \n",
    "    def _get_stats(self):\n",
    "        self.df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b00134fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['High', 'Low', 'Open', 'Close', 'Volume', 'Adj Close', 'company_name']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(h)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(h)\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "print(h)\n",
    "h.remove('Date')\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_schema = StructType([\n",
    "    StructField('Date', DateType(), True),\n",
    "    StructField('High', DoubleType(), True),\n",
    "    StructField('Low', DoubleType(), True),\n",
    "    StructField('Open', DoubleType(), True),\n",
    "    StructField('Close', DoubleType(), True),\n",
    "    StructField('Volume', IntegerType(), True),\n",
    "    StructField('Adj Close', DoubleType(), True),\n",
    "    StructField('company_name', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276a609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN = read_info('stocks_data/AMAZON.csv', header=True, delimiter=',', schema=amzn_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05cbf75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 987\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      "\n",
      "First 40 rows:\n",
      "+----------+-----+-----+-----+-----+------+---------+------------+\n",
      "|      Date| High|  Low| Open|Close|Volume|Adj Close|company_name|\n",
      "+----------+-----+-----+-----+-----+------+---------+------------+\n",
      "|2017-01-03|759.0|759.0|759.0|759.0| 759.0|    759.0|      AMAZON|\n",
      "|2017-01-04|760.0|760.0|760.0|760.0| 760.0|    760.0|      AMAZON|\n",
      "|2017-01-05|782.0|782.0|782.0|782.0| 782.0|    782.0|      AMAZON|\n",
      "|2017-01-06|799.0|799.0|799.0|799.0| 799.0|    799.0|      AMAZON|\n",
      "|2017-01-09|802.0|802.0|802.0|802.0| 802.0|    802.0|      AMAZON|\n",
      "|2017-01-10|798.0|798.0|798.0|798.0| 798.0|    798.0|      AMAZON|\n",
      "|2017-01-11|800.0|800.0|800.0|800.0| 800.0|    800.0|      AMAZON|\n",
      "|2017-01-12|814.0|814.0|814.0|814.0| 814.0|    814.0|      AMAZON|\n",
      "|2017-01-13|822.0|822.0|822.0|822.0| 822.0|    822.0|      AMAZON|\n",
      "|2017-01-17|816.0|816.0|816.0|816.0| 816.0|    816.0|      AMAZON|\n",
      "|2017-01-18|812.0|812.0|812.0|812.0| 812.0|    812.0|      AMAZON|\n",
      "|2017-01-19|814.0|814.0|814.0|814.0| 814.0|    814.0|      AMAZON|\n",
      "|2017-01-20|816.0|816.0|816.0|816.0| 816.0|    816.0|      AMAZON|\n",
      "|2017-01-23|819.0|819.0|819.0|819.0| 819.0|    819.0|      AMAZON|\n",
      "|2017-01-24|824.0|824.0|824.0|824.0| 824.0|    824.0|      AMAZON|\n",
      "|2017-01-25|837.0|837.0|837.0|837.0| 837.0|    837.0|      AMAZON|\n",
      "|2017-01-26|844.0|844.0|844.0|844.0| 844.0|    844.0|      AMAZON|\n",
      "|2017-01-27|840.0|840.0|840.0|840.0| 840.0|    840.0|      AMAZON|\n",
      "|2017-01-30|834.0|834.0|834.0|834.0| 834.0|    834.0|      AMAZON|\n",
      "|2017-01-31|827.0|827.0|827.0|827.0| 827.0|    827.0|      AMAZON|\n",
      "|2017-02-01|834.0|834.0|834.0|834.0| 834.0|    834.0|      AMAZON|\n",
      "|2017-02-02|842.0|842.0|842.0|842.0| 842.0|    842.0|      AMAZON|\n",
      "|2017-02-03|818.0|818.0|818.0|818.0| 818.0|    818.0|      AMAZON|\n",
      "|2017-02-06|811.0|811.0|811.0|811.0| 811.0|    811.0|      AMAZON|\n",
      "|2017-02-07|816.0|816.0|816.0|816.0| 816.0|    816.0|      AMAZON|\n",
      "|2017-02-08|821.0|821.0|821.0|821.0| 821.0|    821.0|      AMAZON|\n",
      "|2017-02-09|825.0|825.0|825.0|825.0| 825.0|    825.0|      AMAZON|\n",
      "|2017-02-10|828.0|828.0|828.0|828.0| 828.0|    828.0|      AMAZON|\n",
      "|2017-02-13|843.0|843.0|843.0|843.0| 843.0|    843.0|      AMAZON|\n",
      "|2017-02-14|838.0|838.0|838.0|838.0| 838.0|    838.0|      AMAZON|\n",
      "|2017-02-15|843.0|843.0|843.0|843.0| 843.0|    843.0|      AMAZON|\n",
      "|2017-02-16|845.0|845.0|845.0|845.0| 845.0|    845.0|      AMAZON|\n",
      "|2017-02-17|847.0|847.0|847.0|847.0| 847.0|    847.0|      AMAZON|\n",
      "|2017-02-21|858.0|858.0|858.0|858.0| 858.0|    858.0|      AMAZON|\n",
      "|2017-02-22|858.0|858.0|858.0|858.0| 858.0|    858.0|      AMAZON|\n",
      "|2017-02-23|861.0|861.0|861.0|861.0| 861.0|    861.0|      AMAZON|\n",
      "|2017-02-24|846.0|846.0|846.0|846.0| 846.0|    846.0|      AMAZON|\n",
      "|2017-02-27|853.0|853.0|853.0|853.0| 853.0|    853.0|      AMAZON|\n",
      "|2017-02-28|854.0|854.0|854.0|854.0| 854.0|    854.0|      AMAZON|\n",
      "|2017-03-01|855.0|855.0|855.0|855.0| 855.0|    855.0|      AMAZON|\n",
      "+----------+-----+-----+-----+-----+------+---------+------------+\n",
      "only showing top 40 rows\n",
      "\n",
      "Last 40 rows:\n",
      "+----------+------+------+------+------+------+---------+------------+\n",
      "|      Date|  High|   Low|  Open| Close|Volume|Adj Close|company_name|\n",
      "+----------+------+------+------+------+------+---------+------------+\n",
      "|2020-12-02|3224.0|3224.0|3224.0|3224.0|3224.0|   3224.0|      AMAZON|\n",
      "|2020-12-01|3249.0|3249.0|3249.0|3249.0|3249.0|   3249.0|      AMAZON|\n",
      "|2020-11-30|3228.0|3228.0|3228.0|3228.0|3228.0|   3228.0|      AMAZON|\n",
      "|2020-11-27|3216.0|3216.0|3216.0|3216.0|3216.0|   3216.0|      AMAZON|\n",
      "|2020-11-25|3198.0|3198.0|3198.0|3198.0|3198.0|   3198.0|      AMAZON|\n",
      "|2020-11-24|3134.0|3134.0|3134.0|3134.0|3134.0|   3134.0|      AMAZON|\n",
      "|2020-11-23|3140.0|3140.0|3140.0|3140.0|3140.0|   3140.0|      AMAZON|\n",
      "|2020-11-20|3133.0|3133.0|3133.0|3133.0|3133.0|   3133.0|      AMAZON|\n",
      "|2020-11-19|3125.0|3125.0|3125.0|3125.0|3125.0|   3125.0|      AMAZON|\n",
      "|2020-11-18|3140.0|3140.0|3140.0|3140.0|3140.0|   3140.0|      AMAZON|\n",
      "|2020-11-17|3189.0|3189.0|3189.0|3189.0|3189.0|   3189.0|      AMAZON|\n",
      "|2020-11-16|3143.0|3143.0|3143.0|3143.0|3143.0|   3143.0|      AMAZON|\n",
      "|2020-11-13|3142.0|3142.0|3142.0|3142.0|3142.0|   3142.0|      AMAZON|\n",
      "|2020-11-12|3176.0|3176.0|3176.0|3176.0|3176.0|   3176.0|      AMAZON|\n",
      "|2020-11-11|3139.0|3139.0|3139.0|3139.0|3139.0|   3139.0|      AMAZON|\n",
      "|2020-11-10|3114.0|3114.0|3114.0|3114.0|3114.0|   3114.0|      AMAZON|\n",
      "|2020-11-09|3289.0|3289.0|3289.0|3289.0|3289.0|   3289.0|      AMAZON|\n",
      "|2020-11-06|3322.0|3322.0|3322.0|3322.0|3322.0|   3322.0|      AMAZON|\n",
      "|2020-11-05|3367.0|3367.0|3367.0|3367.0|3367.0|   3367.0|      AMAZON|\n",
      "|2020-11-04|3245.0|3245.0|3245.0|3245.0|3245.0|   3245.0|      AMAZON|\n",
      "|2020-11-03|3075.0|3075.0|3075.0|3075.0|3075.0|   3075.0|      AMAZON|\n",
      "|2020-11-02|3080.0|3080.0|3080.0|3080.0|3080.0|   3080.0|      AMAZON|\n",
      "|2020-10-30|3167.0|3167.0|3167.0|3167.0|3167.0|   3167.0|      AMAZON|\n",
      "|2020-10-29|3257.0|3257.0|3257.0|3257.0|3257.0|   3257.0|      AMAZON|\n",
      "|2020-10-28|3264.0|3264.0|3264.0|3264.0|3264.0|   3264.0|      AMAZON|\n",
      "|2020-10-27|3292.0|3292.0|3292.0|3292.0|3292.0|   3292.0|      AMAZON|\n",
      "|2020-10-26|3283.0|3283.0|3283.0|3283.0|3283.0|   3283.0|      AMAZON|\n",
      "|2020-10-23|3205.0|3205.0|3205.0|3205.0|3205.0|   3205.0|      AMAZON|\n",
      "|2020-10-22|3199.0|3199.0|3199.0|3199.0|3199.0|   3199.0|      AMAZON|\n",
      "|2020-10-21|3234.0|3234.0|3234.0|3234.0|3234.0|   3234.0|      AMAZON|\n",
      "|2020-10-20|3266.0|3266.0|3266.0|3266.0|3266.0|   3266.0|      AMAZON|\n",
      "|2020-10-19|3329.0|3329.0|3329.0|3329.0|3329.0|   3329.0|      AMAZON|\n",
      "|2020-10-16|3400.0|3400.0|3400.0|3400.0|3400.0|   3400.0|      AMAZON|\n",
      "|2020-10-15|3356.0|3356.0|3356.0|3356.0|3356.0|   3356.0|      AMAZON|\n",
      "|2020-10-14|3465.0|3465.0|3465.0|3465.0|3465.0|   3465.0|      AMAZON|\n",
      "|2020-10-13|3492.0|3492.0|3492.0|3492.0|3492.0|   3492.0|      AMAZON|\n",
      "|2020-10-12|3496.0|3496.0|3496.0|3496.0|3496.0|   3496.0|      AMAZON|\n",
      "|2020-10-09|3289.0|3289.0|3289.0|3289.0|3289.0|   3289.0|      AMAZON|\n",
      "|2020-10-08|3233.0|3233.0|3233.0|3233.0|3233.0|   3233.0|      AMAZON|\n",
      "|2020-10-07|3200.0|3200.0|3200.0|3200.0|3200.0|   3200.0|      AMAZON|\n",
      "+----------+------+------+------+------+------+---------+------------+\n",
      "only showing top 40 rows\n",
      "\n",
      "Missing Data per column:\n",
      "+----+---+----+-----+------+---------+------------+\n",
      "|High|Low|Open|Close|Volume|Adj Close|company_name|\n",
      "+----+---+----+-----+------+---------+------------+\n",
      "|   0|  0|   0|    0|     0|        0|           0|\n",
      "+----+---+----+-----+------+---------+------------+\n",
      "\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------+\n",
      "|summary|              High|               Low|             Open|             Close|           Volume|         Adj Close|company_name|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------+\n",
      "|  count|               987|               987|              987|               987|              987|               987|         987|\n",
      "|   mean|1762.0071216958152|1722.1011452099956|1743.433881363487|1742.9566644206718| 4509728.05775076|1742.9566644206718|        null|\n",
      "| stddev| 667.2385315752688| 644.7988093382758|657.1153070927137| 655.9576061129322|2179817.628631287| 655.9576061129322|        null|\n",
      "|    min|  758.760009765625| 747.7000122070312|757.9199829101562| 753.6699829101562|           881300| 753.6699829101562|      AMAZON|\n",
      "|    25%|            1191.0|            1176.0|1188.300048828125|1186.0999755859375|          2982700|1186.0999755859375|        null|\n",
      "|    50%|   1756.9599609375|  1719.22998046875|1742.239990234375|1739.6500244140625|          3925600|1739.6500244140625|        null|\n",
      "|    75%|1941.5899658203125|1900.3399658203125| 1922.97998046875|  1918.18994140625|          5429100|  1918.18994140625|        null|\n",
      "|    max|           3552.25|  3486.68994140625|           3547.0| 3531.449951171875|         16565000| 3531.449951171875|      AMAZON|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------+\n",
      "\n",
      "None \n",
      "None \n",
      "None\n",
      " None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(AMZN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f70176",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A FAIRE !!!! UN SCHEMA !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.get_df_abstract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.show_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a119abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN._get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ae43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.withColumn(\"test\", \n",
    "              func.datediff(AMZN.df[\"date\"][0], AMZN.df[\"date\"][1])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df[\"Date\"].getItem(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.first()['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec935e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.__get_item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dedf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.second()['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cacb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "func.getrows(AMZN.df, rownums=[0, 2]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df[0].__getitem__(\"Date\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ee9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.select('Date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd646cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n",
    "my_window = Window.partitionBy().orderBy(\"Date\")\n",
    "\n",
    "df = AMZN.df.withColumn(\"prev_value\", F.lag(AMZN.df.Date).over(my_window))\n",
    "df = df.withColumn(\"diff\", F.when(F.isnull(F.datediff(df.Date, df.prev_value)), 0)\n",
    "                              .otherwise(F.datediff(df.Date, df.prev_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d97736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"diff\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(mean('diff')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e623b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.stat.corr('High', 'Low')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001401f",
   "metadata": {},
   "source": [
    "TODO: Create function to compute per month week year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fcbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.select(mean (\"Close\")).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ba4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_mean(df, col):\n",
    "    return df.select(mean (col)).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_col_mean(AMZN.df, \"Close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaafbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.groupBy(func.weekofyear(\"day\").alias(\"date_by_week\")).agg(sum(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.groupBy(func.weekofyear(\"day\").alias(\"date_by_week\")).agg(sum(\"Close\")).orderBy(\"date_by_week\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68793734",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.withColumn(\"Date\",func.date_sub(func.next_day(col(\"Date\"),\"sunday\"),7)).groupBy(\"Date\").agg(sum(\"Close\").cast(\"int\").alias(\"Close_total\")).orderBy(\"week_strt_day\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.groupBy(\"Date\").select(\"Close\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.groupBy(func.month(\"Date\").alias(\"hour\")).agg(mean(\"Close\").alias(\"close_mean\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1411257",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN.df.groupBy(func.year(\"Date\").alias(\"hour\")).agg(mean(\"Close\").alias(\"close_mean\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg(AMZN.df, \"Close\", func.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg(AMZN.df, \"Open\", func.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exploration(object):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def get_oc_avg(self, fun):\n",
    "        close = self._compute_avg(self.df, \"Close\", fun)\n",
    "        opening = self._compute_avg(self.df, \"Open\", fun)\n",
    "\n",
    "        return close.join(\n",
    "            opening, opening.Open_new_time == close.Close_new_time, \"inner\"\n",
    "        ).orderBy(\"Close_new_time\").select(\n",
    "            close.Close_new_time, close.Close_mean, opening.Open_mean\n",
    "        )\n",
    "\n",
    "    def _compute_avg(self, df, col, fun):\n",
    "        return df.groupBy(fun(\"Date\").alias(col + \"_new_time\")).agg(\n",
    "            mean(col).alias(col + \"_mean\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc381123",
   "metadata": {},
   "outputs": [],
   "source": [
    "exAMZN = Exploration(AMZN.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff701f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exAMZN.get_oc_avg(func.month).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exAMZN.get_oc_avg(func.year).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd54380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_change(period=None):\n",
    "    df = AMZN.df\n",
    "    if period:\n",
    "        df= exAMZN.get_oc_avg(period)\n",
    "   \n",
    "    return  df.withColumn('diff', ( df['Close_mean'] - df['Open_mean'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_price_change(func.month).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37884ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_price_change(func.year).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac7ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
