{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#SPARK-:-READ,-TRANSFORM-&amp;-PROCESS-@-SCALE\" data-toc-modified-id=\"SPARK-:-READ,-TRANSFORM-&amp;-PROCESS-@-SCALE-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>SPARK : READ, TRANSFORM &amp; PROCESS @ SCALE</a></span><ul class=\"toc-item\"><li><span><a href=\"#pyspark-official-documentation\" data-toc-modified-id=\"pyspark-official-documentation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>pyspark official documentation</a></span></li><li><span><a href=\"#pyspark-basics\" data-toc-modified-id=\"pyspark-basics-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>pyspark basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-gives-the-possibility-to-read-a-wide-range-of-data-sources-by-natively-supporting-the-read-of-a-huge-number-of-file-types-and-databases\" data-toc-modified-id=\"Spark-gives-the-possibility-to-read-a-wide-range-of-data-sources-by-natively-supporting-the-read-of-a-huge-number-of-file-types-and-databases-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Spark gives the possibility to read a wide range of data sources by natively supporting the read of a huge number of file types and databases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-1-:-read_disp_info()\" data-toc-modified-id=\"Function-1-:-read_disp_info()-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Function 1 : read_disp_info()</a></span></li></ul></li><li><span><a href=\"#Schema-Definition-of-a-DataFrame\" data-toc-modified-id=\"Schema-Definition-of-a-DataFrame-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Schema Definition of a DataFrame</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK : READ, TRANSFORM & PROCESS @ SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1715650187.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    Student Name : Mathieu Rivier\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Student Name : Mathieu Rivier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pyspark official documentation\n",
    "\n",
    "Based on the slides of the course and the spark documentation available online, for each cell, write the spark code that will answer the instructions.\n",
    "\n",
    "You have to follow the tutorials and search over internet in order to get hand's on experience with pyspark, the notebook gives you some suggestions but you are free to try new things and explore the possibilities offered pyspark possibilities.\n",
    "\n",
    "List of tutorial to follow :\n",
    "\n",
    "    - The pyspark API quickstart : https://spark.apache.org/docs/latest/api/python/getting_started/index.html\n",
    "        Please be sure to read the DataFrame API before going further.\n",
    "    - The Spark SQL guide : https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "        This is the most complete guide on how to use Spark SQL. Be sur to do the getting started and the data sources part\n",
    "\n",
    "Important : Create cells to demonstrate the code you write.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pyspark basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_application_name = \"Spark_Application_Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/SPARK/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/06 14:51:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the createDataFrame function, create a spark dataframe with the values [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)] and the columns [\"name\", \"age\"], give this dataframe the name data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"name\", \"age\"]\n",
    "data = [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will show the schema of data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will display 3 values of the dataframe data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "| Denny| 31|\n",
      "| Jules| 30|\n",
      "+------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will return a dataframe with 5 values of the data_df dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Brooke', age=20),\n",
       " Row(name='Denny', age=31),\n",
       " Row(name='Jules', age=30),\n",
       " Row(name='TD', age=35),\n",
       " Row(name='Brooke', age=25)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark gives the possibility to read a wide range of data sources by natively supporting the read of a huge number of file types and databases\n",
    "\n",
    "In the following, write the line of code that will allow you to read the json file specified in the json_file_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"data/devices.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----+--------------------+\n",
      "|dev_type|devnum|    make|model|          release_dt|\n",
      "+--------+------+--------+-----+--------------------+\n",
      "|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|\n",
      "|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|\n",
      "|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|\n",
      "|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|\n",
      "|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|\n",
      "+--------+------+--------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(json_file_path).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will allow you to read a csv file specified in the csv_file_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"data/devices.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------+-----+--------+\n",
      "|devnum|          release_dt|    make|model|dev_type|\n",
      "+------+--------------------+--------+-----+--------+\n",
      "|     1|2008-10-21 00:00:...|Sorrento| F00L|   phone|\n",
      "|     2|2010-04-19 00:00:...| Titanic| 2100|   phone|\n",
      "|     3|2011-02-18 00:00:...|  MeeToo|  3.0|   phone|\n",
      "|     4|2011-09-21 00:00:...|  MeeToo|  3.1|   phone|\n",
      "|     5|2008-10-21 00:00:...|  iFruit|    1|   phone|\n",
      "+------+--------------------+--------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(csv_file_path, sep='\\t', header=True).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 1 : read_disp_info()\n",
    "Write a function, named read_disp_info, which takes as input a file path (json or csv path) and give the following information : It creates a dataframe from the file path, display it's schema, display the 10 first line of the dataframe and give the total number of rows of the dataframe\n",
    "It is important to understand that this function should be able to read the json and csv files, to do this, it can look at the end of filepath string in order to deduce if the extension of the file is csv or json and then call the reading function accordinly to the extension. At the end, this function return a spark dataframe object representing the read data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_csv(csv_file_path):\n",
    "    return spark.read.csv(csv_file_path, sep='\\t', header=True)\n",
    "\n",
    "\n",
    "def _handle_json(json_file_path):\n",
    "    return spark.read.json(json_file_path)\n",
    "\n",
    "def _load_df(file_path):\n",
    "    extension = file_path.split(\".\")[-1]\n",
    "    \n",
    "    df = None\n",
    "    if extension == 'json':\n",
    "        df = _handle_json(file_path)\n",
    "    elif extension == 'csv':\n",
    "        df = _handle_csv(file_path)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def read_disp_info(file_path):\n",
    "    # load df\n",
    "    df = _load_df(file_path)\n",
    "    \n",
    "    # Print Schema\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Show the First 10 lines\n",
    "    df.show(10)\n",
    "    \n",
    "    # Number of total rows\n",
    "    print(\"Number of rows: \" + str(df.count()))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, read the file present in \"/home/adminefrei/Documents/data/devices.json\" and affect the retuned dataframe to the variable devDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dev_type: string (nullable = true)\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: string (nullable = true)\n",
      "\n",
      "+--------+------+--------+-----+--------------------+\n",
      "|dev_type|devnum|    make|model|          release_dt|\n",
      "+--------+------+--------+-----+--------------------+\n",
      "|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|\n",
      "|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|\n",
      "|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|\n",
      "|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|\n",
      "|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|\n",
      "|   phone|     6|  iFruit|    3|2011-11-02T00:00:...|\n",
      "|   phone|     7|  iFruit|    2|2010-05-20T00:00:...|\n",
      "|   phone|     8|  iFruit|    5|2013-07-02T00:00:...|\n",
      "|   phone|     9| Titanic| 1000|2008-10-21T00:00:...|\n",
      "|   phone|    10|  MeeToo|  1.0|2008-10-21T00:00:...|\n",
      "+--------+------+--------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of rows: 50\n"
     ]
    }
   ],
   "source": [
    "devDF = read_disp_info(\"data/devices.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame transformations typically return another DataFrame. Try using a select to return a DataFrame with only the make and model columns, name this dataframe makeModelDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeModelDF = devDF.select('make', 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the schema of the makeModelDF, what can you deduce ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "makeModelDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have the 'Make' and 'Model' columns left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations in a query can be chained together. Write the line of code that will run a query using select the columns devnum, make, and model and where the make is equal to \"Ronin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------+\n",
      "|devnum| make|         model|\n",
      "+------+-----+--------------+\n",
      "|    15|Ronin|Novelty Note 1|\n",
      "|    17|Ronin|Novelty Note 3|\n",
      "|    18|Ronin|Novelty Note 2|\n",
      "|    19|Ronin|Novelty Note 4|\n",
      "|    46|Ronin|            S4|\n",
      "|    47|Ronin|            S1|\n",
      "|    48|Ronin|            S3|\n",
      "|    49|Ronin|            S2|\n",
      "+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devDF.select('devnum', 'make', 'model').filter(devDF['make'] == 'Ronin').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can also read parquet files, parquet is one of the most used format in hdfs, search in the doc and write the line of the code that will let you read the parquet file present in parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"data/base_stations.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_station = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, select the id, lat and lon columns where the city = Scottsdale from this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| id|    lat|      lon|\n",
      "+---+-------+---------+\n",
      "| 14|33.6165|-111.9554|\n",
      "| 15|33.6968|-111.8892|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_base_station.filter(df_base_station['city'] == 'Scottsdale').select('id', 'lat', 'lon').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Definition of a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new DataFrame based on the devices.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dev_type: string (nullable = true)\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: string (nullable = true)\n",
      "\n",
      "+--------+------+--------+-----+--------------------+\n",
      "|dev_type|devnum|    make|model|          release_dt|\n",
      "+--------+------+--------+-----+--------------------+\n",
      "|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|\n",
      "|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|\n",
      "|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|\n",
      "|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|\n",
      "|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|\n",
      "|   phone|     6|  iFruit|    3|2011-11-02T00:00:...|\n",
      "|   phone|     7|  iFruit|    2|2010-05-20T00:00:...|\n",
      "|   phone|     8|  iFruit|    5|2013-07-02T00:00:...|\n",
      "|   phone|     9| Titanic| 1000|2008-10-21T00:00:...|\n",
      "|   phone|    10|  MeeToo|  1.0|2008-10-21T00:00:...|\n",
      "+--------+------+--------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of rows: 50\n"
     ]
    }
   ],
   "source": [
    "devDF = read_disp_info('data/devices.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the schema of the devDF DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the release_dt column is of type string, whereas the data in the column actually represents a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done automatically by the function :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a schema that correctly specifies the column types for this DataFrame starts by importing the package with the definitions of necessary classes and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a collection of StructField objects, which represent column definitions. The release_dt column will be a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "devColumns = [StructField(\"devnum\",LongType()), StructField(\"make\",StringType()), StructField(\"model\",StringType()), StructField(\"release_dt\",TimestampType()),StructField(\"dev_type\",StringType())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a schema (a StructType object) using the column definition list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "devSchema = StructType(devColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you have created the devSchema, search in the doc how you can specify the schema when you create the dataframe from the json_file_path (\"/path/to/devices.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(devnum=1, make='Sorrento', model='F00L', release_dt='2008-10-21T00:00:00.000-07:00', dev_type='phone')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devDF.select('devnum', 'make', 'model', 'release_dt', 'dev_type').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = spark.read.schema(devSchema).json('data/devices.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the schema and data of the new DataFrame, and confirm that the release_dt column type is now timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: timestamp (nullable = true)\n",
      " |-- dev_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-------------------+--------+\n",
      "|devnum|    make|model|         release_dt|dev_type|\n",
      "+------+--------+-----+-------------------+--------+\n",
      "|     1|Sorrento| F00L|2008-10-21 09:00:00|   phone|\n",
      "|     2| Titanic| 2100|2010-04-19 09:00:00|   phone|\n",
      "|     3|  MeeToo|  3.0|2011-02-18 09:00:00|   phone|\n",
      "|     4|  MeeToo|  3.1|2011-09-21 09:00:00|   phone|\n",
      "|     5|  iFruit|    1|2008-10-21 09:00:00|   phone|\n",
      "|     6|  iFruit|    3|2011-11-02 08:00:00|   phone|\n",
      "|     7|  iFruit|    2|2010-05-20 09:00:00|   phone|\n",
      "|     8|  iFruit|    5|2013-07-02 09:00:00|   phone|\n",
      "|     9| Titanic| 1000|2008-10-21 09:00:00|   phone|\n",
      "|    10|  MeeToo|  1.0|2008-10-21 09:00:00|   phone|\n",
      "+------+--------+-----+-------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will read the account_device_path correctly, search how to set up the delimiter and consider the header when reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_device_path = \"data/accountdevice/part-00000-f3b62dad-1054-4b2e-81fd-26e54c2ae76a.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+---------------+--------------------+\n",
      "|   id|account_id|device_id|activation_date|   account_device_id|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|\n",
      "|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|\n",
      "|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(account_device_path, sep=',', header=True).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the function read_disp_info(), and add to it two optional parameters (header and delimiter) those two optional paramaters can be used optionnally and have the default values header = False and delimiter = \";\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_csv(csv_file_path, header, delimiter):\n",
    "    return spark.read.csv(csv_file_path, sep=delimiter, header=header)\n",
    "\n",
    "\n",
    "def _handle_json(json_file_path):\n",
    "    return spark.read.json(json_file_path)\n",
    "\n",
    "\n",
    "def _load_df(file_path, header, delimiter):\n",
    "    extension = file_path.split(\".\")[-1]\n",
    "\n",
    "    df = None\n",
    "    if extension == 'json':\n",
    "        df = _handle_json(file_path)\n",
    "    elif extension == 'csv':\n",
    "        df = _handle_csv(file_path, header, delimiter)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def updated_read_disp_info(file_path, header=False, delimiter=';'):\n",
    "    df = _load_df(file_path, header, delimiter)\n",
    "\n",
    "    # Print Schema\n",
    "    df.printSchema()\n",
    "\n",
    "    # Show the First 10 lines\n",
    "    df.show(10)\n",
    "\n",
    "    # Number of total rows\n",
    "    print(\"Number of rows: \" + str(df.count()))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the function defined, test it to read the account_device_path file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- activation_date: string (nullable = true)\n",
      " |-- account_device_id: string (nullable = true)\n",
      "\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|   id|account_id|device_id|activation_date|   account_device_id|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|\n",
      "|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|\n",
      "|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|\n",
      "|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|\n",
      "|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|\n",
      "|48697|     32447|        6|  1342578469000|cc8e8361-3d67-4be...|\n",
      "|48698|     32447|       29|  1386643231000|b40dba90-b073-405...|\n",
      "|48699|     32448|        5|  1350883104000|f088d30f-1e1c-47f...|\n",
      "|48700|     32448|       29|  1383321310000|2805df93-2e89-433...|\n",
      "|48701|     32449|       34|  1333225574000|e0e7edbe-77fc-421...|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of rows: 194764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, account_id: string, device_id: string, activation_date: string, account_device_id: string]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_read_disp_info(account_device_path, True, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d31793bc0164bbf1a7493650a68c8e00179ba062bd5a26b747d14cdd740a9f47"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
